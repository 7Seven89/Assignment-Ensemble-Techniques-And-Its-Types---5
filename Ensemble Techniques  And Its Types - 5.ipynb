{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2d8380-851a-4364-9d16-9ad425bf6fe5",
   "metadata": {},
   "source": [
    "### Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "- **Use an automated feature selection method** to identify the important features in the dataset.\n",
    "- **Create a numerical pipeline** that includes the following steps:\n",
    "  - Impute the missing values in the numerical columns using the mean of the column values.\n",
    "  - Scale the numerical columns using standardisation.\n",
    "- **Create a categorical pipeline** that includes the following steps:\n",
    "  - Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "  - One-hot encode the categorical columns.\n",
    "- **Combine the numerical and categorical pipelines** using a `ColumnTransformer`.\n",
    "- **Use a Random Forest Classifier** to build the final model.\n",
    "- **Evaluate the accuracy** of the model on the test dataset.\n",
    "\n",
    "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290742e-94a1-4f58-bb6e-490e8a290224",
   "metadata": {},
   "source": [
    "## Q1. Design a Pipeline for Feature Engineering and Model Building\n",
    "\n",
    "In this task, we are working on a machine learning project that involves a dataset with both numerical and categorical features. The goal is to automate the feature engineering process and handle missing values. We will use a pipeline to automate the steps for data preprocessing and model building. Hereâ€™s how to do it:\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "We will start by importing the necessary libraries for data manipulation, preprocessing, and modeling.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16021e-cde2-44f6-8ef0-9f352c4f44d9",
   "metadata": {},
   "source": [
    "## Step 2: Load the Data\n",
    "Assume the dataset is already loaded into a DataFrame df. For this example, we assume that the target variable is target, and other columns include both numerical and categorical features.\n",
    "\n",
    "```python\n",
    "# Example dataset\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb9ec6-0323-4df9-bb40-751376322d74",
   "metadata": {},
   "source": [
    "## Step 3: Identify Numerical and Categorical Features\n",
    "We separate the features into numerical and categorical columns to apply appropriate preprocessing steps.\n",
    "\n",
    "```python\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212c32b-976d-42b5-87b1-3423a9a2b9d4",
   "metadata": {},
   "source": [
    "## Step 4: Create the Numerical Pipeline\n",
    "The numerical pipeline includes:\n",
    "- **Imputation:** Fill missing values in numerical columns with the mean of the column values.\n",
    "- **Scaling:** Standardize the numerical columns using StandardScaler.\n",
    "\n",
    "```python\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f89d75-d824-4d2d-a6de-f56963e7f8e6",
   "metadata": {},
   "source": [
    "## Step 5: Create the Categorical Pipeline\n",
    "The categorical pipeline includes:\n",
    "- **Imputation:** Fill missing values in categorical columns with the most frequent value of the column.\n",
    "- **One-hot Encoding:** Convert categorical variables into a format that can be provided to ML models.\n",
    "\n",
    "```python\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode the categorical features\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf45b9-9bb4-4ce3-a848-88ac87e88953",
   "metadata": {},
   "source": [
    "## Step 6: Combine the Pipelines Using ColumnTransformer\n",
    "The ColumnTransformer is used to apply the numerical and categorical pipelines to the respective columns.\n",
    "\n",
    "```python\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1fb92-0361-4e00-9a65-f03f5ef09cac",
   "metadata": {},
   "source": [
    "## Step 7: Feature Selection Using SelectKBest\n",
    "We will apply automated feature selection using the SelectKBest method with the f_classif statistical test. This step selects the best features based on their relationship with the target variable.\n",
    "\n",
    "```python\n",
    "selector = SelectKBest(f_classif, k='all')  # Select all features (you can adjust k to select top k features)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f704ab0-3e5d-4a36-a6aa-cfa3d2121da1",
   "metadata": {},
   "source": [
    "## Step 8: Build the Model Pipeline\n",
    "We combine the feature selection step and the model (Random Forest Classifier) into a final pipeline.\n",
    "\n",
    "```python\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', selector),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c352d-143d-49a5-8f76-465b4bf45760",
   "metadata": {},
   "source": [
    "## Step 9: Train-Test Split\n",
    "We split the data into training and test sets.\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a1fa3-1b45-4390-bad3-5794c17702cf",
   "metadata": {},
   "source": [
    "## Step 10: Train the Model\n",
    "We train the pipeline on the training dataset.\n",
    "\n",
    "```python\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a15cbc-b162-44ee-afba-e25286e8c003",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate the Model\n",
    "Finally, we evaluate the accuracy of the model on the test set.\n",
    "\n",
    "```python\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Random Forest model: {accuracy}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2ccb9-633e-4a7e-8471-b0f0fca840bc",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "- The Random Forest model uses an ensemble of decision trees, which helps in reducing overfitting compared to a single decision tree.\n",
    "- Feature selection ensures that we are using the most important features, which can improve model performance by reducing noise.\n",
    "- Preprocessing steps like imputation and scaling are crucial for handling missing values and ensuring the numerical features are on the same scale.\n",
    "\n",
    "## Possible Improvements:\n",
    "- Feature Selection: We can fine-tune the number of features selected using SelectKBest.\n",
    "- Hyperparameter Tuning: Tune the hyperparameters of the Random Forest model using techniques like GridSearchCV or RandomizedSearchCV.\n",
    "- Imputation Strategy: Explore other imputation strategies, like using a median or applying a more sophisticated method like KNN imputation.\n",
    "- Model Selection: Test other models (e.g., XGBoost, SVM) for comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eee84-5275-4331-ae7e-fc7a60fb883c",
   "metadata": {},
   "source": [
    "## Q2. Build a Pipeline with Random Forest Classifier and Logistic Regression Classifier Using a Voting Classifier\n",
    "In this task, we will create a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and we will combine their predictions using a Voting Classifier. We will train this pipeline on the Iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65166216-3d4c-49f3-bcbf-a40d0e264142",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries for the Voting Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53721b81-4aa3-45ca-aa7d-1a8f45a3a504",
   "metadata": {},
   "source": [
    "## Step 2: Load the Iris Dataset\n",
    "\n",
    "```python\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d8d85-6c5e-4905-981a-2e360eaa0224",
   "metadata": {},
   "source": [
    "## Step 3: Split the Data into Train and Test Sets\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa839c-c20e-4b19-a975-e9ecb2cd6e4d",
   "metadata": {},
   "source": [
    "## Step 4: Create the Random Forest and Logistic Regression Classifiers\n",
    "\n",
    "```python\n",
    "# Define the classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44e88e-8f0a-493a-8d72-69102fbe0d5d",
   "metadata": {},
   "source": [
    "## Step 5: Create the Voting Classifier\n",
    "We will create a VotingClassifier that combines the predictions of the Random Forest and Logistic Regression classifiers.\n",
    "\n",
    "```python\n",
    "# Create a Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='hard')  # 'hard' voting means majority rule\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500a06-bbd0-40af-a8d7-bb000a06be57",
   "metadata": {},
   "source": [
    "## Step 6: Create the Pipeline\n",
    "Now we will create a pipeline that includes the voting classifier.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', voting_classifier)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cbe93-7c74-412c-8202-6a532b5b03f3",
   "metadata": {},
   "source": [
    "## Step 7: Train the Pipeline\n",
    "We will train the pipeline on the training dataset.\n",
    "\n",
    "```python\n",
    "pipeline.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc480e-52a4-41bb-a034-f4f16a70ca31",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the Model\n",
    "Finally, we evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "```python\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the Voting Classifier model: {accuracy}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85938759-a260-47ab-8b44-af325292eda6",
   "metadata": {},
   "source": [
    "## Interpretation of Results:\n",
    "- Voting Classifier: By combining the predictions of two classifiers (Random Forest and Logistic Regression), the model benefits from both classifiers' strengths.\n",
    "- Hard Voting: In hard voting, the class label that gets the majority of votes from the individual models is chosen as the final prediction.\n",
    "- Accuracy: The accuracy score will tell us how well the ensemble of classifiers performs compared to each individual classifier.\n",
    "\n",
    "## Possible Improvements:\n",
    "- Hyperparameter Tuning: Tune the hyperparameters of the classifiers to improve performance.\n",
    "- Soft Voting: Experiment with soft voting, where the class probabilities are averaged rather than voting on the most frequent class.\n",
    "- Add More Classifiers: We could add other classifiers (e.g., SVM, KNN) to the voting classifier to improve predictions further.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
